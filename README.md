# signate_smbc_40th_place_solution

## 最初に
- はじめにSIGNATEと主催者の方々ありがとうございました。
- f1スコアが低めでかつ極端な不均衡データであったため、もしかしたら荒れるかなと思ってました。
    - 早い段階でモデリングに見切りをつけて、特徴量選択に時間割けた点は良かった点
- 自分は運よく150位くらいshakeupしました。

- コードは以下です。（最終提出とは異なる）
https://github.com/monaou/signate_smbc_40th_place_solution

## やったこと

### 問題設定
- マルチクラス分類

### モデル
- LightGBM

### CV
- StratifiedKFold (n=5)
- MetricはMacroF1を設定。それぞれeary_stopを使用。
- class weightは設定。重みはTrainデータの比率。

### 特徴量
- Problem、spc commonはラベルエンコーディング＋種類ごとのワンホットエンコーディング。
    - oak, mapleなどの種類毎の傾向が取れることを期待した。
- Problemは種類数も使用。
- Staffなどはワンホットエンコーディング。
- 緯度、経度。

### 後処理
- 単純なアンサンブル。ラベル付けはargmax。

## 効かなかったこと
- 不均衡データ対策は色々検討したが結局ほとんど採用しなかった。

### 問題設定
- 2値分類の結果を特徴量化。
  - Fair/badだけを分類するモデルを作り、特徴量として使う構成
    - CVが不安定かつpublicと相関せず不採用
- アンダー/オーバーサンプリング
  - SMOTEなどによりデータセットの比を調整。
    - 想定通り性能伸びず不採用。（SMOTEで性能伸びたことない、、）
- 不均衡データを考慮した損失関数
  - 損失関数をFocal Loss等に変更した
    - 実装が怪しいのもあるが、学習があまり進まず。性能も微妙で不採用

### 特徴量
- 日付（正確にはunixtimeに変換）はCVでは上がるがpublicと相関せず不採用
- spc common毎のtree_dbhの集約特徴量。

### モデル
- liner/Radam Forest/cat/xgb全て試したが、一番性能よかったLightGBMを採用

### アンサンブル
- 複数パラメータ種類のLightGBM後に他のモデルでスタッキングするパターンなどを検討
    - CV上はスタッキングの効果が少なかったので最良パラメータLightGBMだけを使いOOFのアンサンブルだけする形にした

## 残課題
- TTAの工夫が足りなかった。
  - argmaxによる推論結果のラベル付けなどやスタッキングなど

- ターゲットエンコーディングはCVで効果がなく不採用としたが、もう少し詰めても良かった
